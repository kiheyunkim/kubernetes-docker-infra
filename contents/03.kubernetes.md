#### kubelet : 쿠버네티스에서 파드의 생성과 상태 관리 및 복구 등을 담당하는 구성 요소
kubelet 서비스를 종료 시키는 경우 pod 생성 관리등이 정상적으로 작동하지 않음을 마스터노드에서 볼 수 있음

#### kube-proxy: kube-proxy는 파트의 통신을 담당함.

#### kubernetes

##### 파드를 실행하는 방법
```shell
# kubectl run을 통해서 파드를 실행
kubectl run nginx-pod --image=nginx
```

```shell
kubectl create nginx --image=nginx

Error: unknown flag: --image
See 'kubectl create --help' for usage. 
# create에는 이미지 옵션이 없음.

# create에 이미지를 지정하기 위해서는 deployment로 생성해야 함
kubectl create deployment <deployment name> --image=nginx
```

```shell
# deployment의 pod를 3개로 증가 시킴
kubectl scale deployment <deployment name> --replicas=3
```
3개의 nginx pod를 디플로이먼트 오브젝트로 만드는 방법
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: echo-hname
  labels:
    app: nginx
spec:
  replicas: 6
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: echo-hname
        image: sysnet4admin/echo-hname
```
```shell
kubectl create -f xxxx.yml
```

yaml 파일을 통해서 파드를 늘리는 방법
```shell
kubectl create -f xxxx.yml

Error from server (AlreadyExists): error when creating "/root/_Book_k8sInfra/ch3/3.2.4/echo-hname.yaml": deployments.apps "echo-hname" already 
# 이미 있으므로 만들 수 없다는 경고가 뜸
```

apply 통해서 한다면
```shell
kubectl apply -f xxxx.yml

Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
deployment.apps/echo-hname configured

#처음부터 apply로 생성한 것이 아니어서 경고가 뜸. 작동에는 이상없지만 일관성에 문제가 생김
```

따라서 변경 가능성이 있는 복잡한 오브젝트는 파일로 작성한 후 apply로 적용하는 것이 좋고, 애드훅으로 오브젝트를 생성할 떄는 create로 사용하는 것이 좋음

| 구분     | RUN  | CREATE | APPLY    |
|--------|------|--------|----------|
| 명령 실행  | 제한적  | 가능함    | 안 됨      |
| 파일 실행  | 안 됨  | 가능함    | 가능함      |
| 변경 가능  | 안 됨  | 안 됨    | 가능함      |
| 실행 편의성 | 매우 좋음 | 매우 좋음  | 좋음       |
| 기능 유지  | 제한적  | 지원함    | 다양하게 지원함 |


##### 파드의 컨테이너 자동 복구

```shell
kubectl delete pods nginx-pods
# 디플로이먼트에 속한 파드도 아니고 어떤 컨트롤러도 이 파드를 관리 하지 않음 => 바로 삭제되고 바로 생성됨

kubectl delete pods echo-hname-7894b67f-24r57
# 디플로이먼트에 의해 관리 되고 있으므로 다른 이름으로 재 생성됨(deployment에서 replicas =6으로 선언했기 때문에 6개로 맞추기 위해 서)

kubectl delete deployment echo-hname
# 디플로이먼트에 속한 파드는 상위 디플로이먼트를 삭제해야 파드가 삭제됨.
``` 

##### 노드 자원 보호하기

```shell
# get pods 커스터마이징을 통해서 가져오기
kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName
NAME


NAME                        IP               STATUS    NODE
echo-hname-7894b67f-65jfn   172.16.103.133   Running   w2-k8s
echo-hname-7894b67f-ckpfk   172.16.103.135   Running   w2-k8s
echo-hname-7894b67f-gjcsb   172.16.221.136   Running   w1-k8s
echo-hname-7894b67f-gpdmp   172.16.132.9     Running   w3-k8s
echo-hname-7894b67f-h5qfs   172.16.221.135   Running   w1-k8s
echo-hname-7894b67f-jsb7r   172.16.132.8     Running   w3-k8s
echo-hname-7894b67f-pn8gs   172.16.103.134   Running   w2-k8s
echo-hname-7894b67f-rks7w   172.16.132.7     Running   w3-k8s
echo-hname-7894b67f-vtlzb   172.16.221.137   Running   w1-k8s
```

```shell
# 노드에 문제가 자주 발생하는 경우 현재 상태를 보존해야할때 cordon 명령어를 실행함
kubectl cordon w3-k8s

node/w3-k8s cordoned

# w3-k8s에는 더이상 파드가 할당되지 않는 상태로 변경됨. 
kubectl get nodes

NAME     STATUS                     ROLES    AGE   VERSION
m-k8s    Ready                      master   45h   v1.18.4
w1-k8s   Ready                      <none>   44h   v1.18.4
w2-k8s   Ready                      <none>   44h   v1.18.4
w3-k8s   Ready,SchedulingDisabled   <none>   44h   v1.18.4

# pod 수를 증량시키는 경우 w3-k8s에서는 제외됨

NAME                        IP               STATUS    NODE
echo-hname-7894b67f-65jfn   172.16.103.133   Running   w2-k8s
echo-hname-7894b67f-744ng   172.16.221.138   Running   w1-k8s
echo-hname-7894b67f-cv4jn   172.16.103.137   Running   w2-k8s
echo-hname-7894b67f-dw64h   172.16.221.140   Running   w1-k8s
echo-hname-7894b67f-dxrq2   172.16.221.139   Running   w1-k8s
echo-hname-7894b67f-h5qfs   172.16.221.135   Running   w1-k8s
echo-hname-7894b67f-nthtd   172.16.103.136   Running   w2-k8s
echo-hname-7894b67f-rks7w   172.16.132.7     Running   w3-k8s
echo-hname-7894b67f-xgx47   172.16.103.138   Running   w2-k8s

# uncordon을 통해서 할당이 다시 되도록 할 수 있음
kubectl uncordon w3-k8s

node/w3-k8s uncordoned
```

##### 노드 유지보수
유지보수를 위해 노드를 꺼야하는 상황이 발생함. drain 기능을 통해 지정된 노드의 파드를 전부 다른 곳으로 이동시킬 수 있음

pod는 언제든 삭제되고 다시 생성할 수 있기 떄문에 실제로는 옮기지는 않고 새로 생성함

하지만 DaemontSet은 노드에 하나만 존재하는 파드라서 drain으로는 삭제할 수 없음

```shell
# drain 실행
kubectl drain w3-k8s --ignore-daemonsets

WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-4tclx, kube-system/kube-proxy-z7pnv
evicting pod default/echo-hname-7894b67f-rks7w
pod/echo-hname-7894b67f-rks7w evicted
node/w3-k8s evicted
```

```shell
# drain을 사용하면 cordon명령어를 실행된 것 처럼 바뀜
NAME     STATUS                     ROLES    AGE   VERSION
m-k8s    Ready                      master   45h   v1.18.4
w1-k8s   Ready                      <none>   45h   v1.18.4
w2-k8s   Ready                      <none>   45h   v1.18.4
w3-k8s   Ready,SchedulingDisabled   <none>   45h   v1.18.4
```

##### pod 업데이트하고 복구
```shell
# 파드 업데이트
kubectl apply -f ~/_Book_k8sInfra/ch3/3.2.10/rollout-nginx.yaml  --record
# record 옵션을 사용하면 배포한 정보의 히스토리를 기록함.

kubectl rollout history deployment rollout-nginx
# rollout history 명령어를 통해서 히스토리를 확인할 수 있음
deployment.apps/rollout-nginx
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=/root/_Book_k8sInfra/ch3/3.2.10/rollout-nginx.yaml --record=true


# 파드 업데이트
 kubectl set image deployment rollout-nginx nginx=nginx:1.16.0 --record
# 파드는 하나하나 재생성함. 따라서 이름과 IP가 변경됨. 업데이트는 전체의 25%씩 임. 최소 값은 1개

# deployment 상태 확인
kubectl rollout status deployment rollout-nginx

deployment.apps/rollout-nginx
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=/root/_Book_k8sInfra/ch3/3.2.10/rollout-nginx.yaml --record=true
2         kubectl set image deployment rollout-nginx nginx=nginx:1.16.0 --record=true

# 실패 상황 만들기 nginx 컨테이너를 버젼 1.17.23으로 변경
kubectl set image deployment rollout-nginx nginx=nginx:1.17.23 --record
deployment.apps/rollout-nginx image updated

# Pending 상태에서 정지 되어 있음
kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName

NAME                             IP               STATUS    NODE
rollout-nginx-8566d57f75-4gtzs   172.16.221.143   Running   w1-k8s
rollout-nginx-8566d57f75-z9lkg   172.16.103.141   Running   w2-k8s
rollout-nginx-8566d57f75-zv2tj   172.16.103.142   Running   w2-k8s
rollout-nginx-856f4c79c9-wbx8j   172.16.132.10    Pendin

# rollout status로 확인한 상태
kubectl rollout status deployment rollout-nginx
Waiting for deployment "rollout-nginx" rollout to finish: 1 out of 3 new replicas have been updated...

# 조금더 기다리면 시도후 생성 실패했다고 뜬다
kubectl rollout status deployment rollout-nginx
error: deployment "rollout-nginx" exceeded its progress deadline

# describe 명령어로 확인
kubectl describe deployment rollout-nginx

... 
OldReplicaSets:  rollout-nginx-8566d57f75 (3/3 replicas created)
NewReplicaSet:   rollout-nginx-856f4c79c9 (1/1 replicas created)
...
# 새로 생성 되는 과정에서 멈춰 있는 것을 확인할 수 있음.

# 업데이트 할 때 사용했던 명령어들을 rollout history로 확인
kubectl rollout history deployment rollout-nginx
deployment.apps/rollout-nginx
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=/root/_Book_k8sInfra/ch3/3.2.10/rollout-nginx.yaml --record=true
2         kubectl set image deployment rollout-nginx nginx=nginx:1.16.0 --record=true
3         kubectl set image deployment rollout-nginx nginx=nginx:1.17.23 --record=true

# 마지막 단계(rev 3)에서 전 단계(rev 2)로 되돌리면 파드가 정상으로 돌고 있음을 확인할 수 있음
kubectl get pods -o=custom-columns=NAME:.metadata.name,IP:.status.podIP,STATUS:.status.phase,NODE:.spec.nodeName

NAME                             IP               STATUS    NODE
rollout-nginx-8566d57f75-4gtzs   172.16.221.143   Running   w1-k8s
rollout-nginx-8566d57f75-z9lkg   172.16.103.141   Running   w2-k8s
rollout-nginx-8566d57f75-zv2tj   172.16.103.142   Running   w2-k8s

# rollout history로 확인하면 다음과 같으며
kubectl rollout history deployment rollout-nginx

deployment.apps/rollout-nginx
REVISION  CHANGE-CAUSE
1         kubectl apply --filename=/root/_Book_k8sInfra/ch3/3.2.10/rollout-nginx.yaml --record=true
3         kubectl set image deployment rollout-nginx nginx=nginx:1.17.23 --record=true
4         kubectl set image deployment rollout-nginx nginx=nginx:1.16.0 --record=true

# rev2로 되돌렸기 떄문에 rev2는 삭제되고 rev4가 추가되있음. 그리고 가장 최근 상태는 rev4임

#특정 rev로 돌아가는 방법
kubectl rollout undo deployment rollout-nginx --to-revision=1
```

#### 쿠버네티스 연결을 담당하는 서비스

##### 노드포트
쿠버네티스 클러스터의 내부에 접속하는 쉬운 방법은 노드포트. 특정 포트를 열고 모든 워커 노드의 특정 포트를 열고 모든 요청을 노드포트 서비스로 전달함.

노드 포트 서비스는 해당 업무를 처리할 수 있는 파드로 요청을 전달함.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: np-svc
spec:
  selector: # 컨테이너 정보 없이 접속에 필요한 네트워크 관련 정보와 서비스 타입을 지정함.
    app: np-pods 
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30000
  type: NodePort
```

```shell
kubectl create -f ~/_Book_k8sInfra/ch3/3.3.1/nodeport.yaml
```
```shell
# get services를 하게되면 
kubectl get services

NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP        2d2h
np-svc       NodePort    10.102.128.114   <none>        80:30000/TCP   42m
```

노드포트의 오브젝트 스펙에 적힌 np-pods와 deployment의 이름을 확인해 동일하면 같은 파드라고 간주하고 외부에서 추적해 접속함.

##### expose로 노드포트 서비스 생성

```shell
kubectl expose deployment np-pods --type=NodePort ==name=np-svc-v2 --port=80
```

##### 사용 목적별로 연결하는 인그레이스

노드포트 서비스는 포트를 중복사용할 수 없어서 1개의 노드포트에 1개의 디플로이먼트만 적용됨. 인그레이스를 사용하면 고유한 주소를 제공해 사용 목적에 따라 다른 응답을 제공할 수 있고, 트래픽에 대한 L4/L7
로드밸런서와 바안 인증서를 처리하는 기능을 제공함.

**NGINX 인그레이스 컨트롤러 구성**

1. 사용자는 노드마다 설정된 노드포트를 통해 노드포트 서비스로 접속함. 이때 노드포트 서비스를 NGINX인그레이스 컨트롤러로 구성함. 
2. NGINX 인그레이스 컴트롤러는 사용자의 접속 경로에 따라 적합한 클러스터 IP 서비스로 경로를 제공함
3. 클러스터 IP 서비스는 사용자를 해당 파드로 연결해줌

```shell
# 테스트 디플로이먼트 2개 배포
kubectl create deployment in-hname-pod --image=sysnet4admin/echo-hname

deployment.apps/in-hname-pod created

kubectl create deployment in-ip-pod --image=sysnet4admin/echo-ip

deployment.apps/in-ip-pod created

kubectl get pods

NAME                            READY   STATUS              RESTARTS   AGE
in-hname-pod-8565c86448-pjpmq   1/1     Running             0          21s
in-ip-pod-76bf6989d-vz9wq       0/1     ContainerCreating   0          3s
```

NGINX INGRESS 컨트롤러 설치
```yaml
# All of sources From https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml
# clone from above to sysnet4admin 

apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
    verbs:
      - update

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
    spec:
      # wait up to five minutes for the drain of connections
      terminationGracePeriodSeconds: 300
      serviceAccountName: nginx-ingress-serviceaccount
      nodeSelector:
        kubernetes.io/os: linux
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            # www-data -> 101
            runAsUser: 101
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          lifecycle:
            preStop:
              exec:
                command:
                  - /wait-shutdown

---

apiVersion: v1
kind: LimitRange
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  limits:
  - min:
      memory: 90Mi
      cpu: 100m
    type: Container
```


```shell
# 인그레이스 컨트롤러 배포 확인, NGINX 인그레이스 컴트롤러는 default 네임 스페이스가 아닌 ingree-nginx 네임스페이스에 속함
# 서비스 확인시에도 동일한 네임스페이스를 줘야함
kubectl get pods -n ingress-nginx

NAME                                        READY   STATUS    RESTARTS   AGE
nginx-ingress-controller-5bb8fb4bb6-5hz96   1/1     Running   0          58s
```

인그레이스를 사용자 요구 사항에 맞게 설정하려면 경로와 작동을 정의해야함.
```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: ingress-nginx #ingress 이름
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: / # rewrite-taget을 지정
spec:
  rules:
  - http:
      paths:
      - path:
        backend:
          serviceName: hname-svc-default
          servicePort: 80
      - path: /ip
        backend:
          serviceName: ip-svc
          servicePort: 80
      - path: /your-directory
        backend:
          serviceName: your-svc
          servicePort: 80
```

```shell
# 인그레이스 설정 파일이 제대로 등록됐는지 확인
kubectl get ingress

NAME            CLASS    HOSTS   ADDRESS   PORTS   AGE
ingress-nginx   <none>   *                 80      34m

# yaml형식으로 출력
kubectl get ingress -o yaml
```

NGINX 인그레스 컨트롤러 생성과 인그레스 설정을 완료했고. 외부에서 NGINX 인그레스 컨트롤러에 접속할 수 있게 노드포트 서비스로 NGINX 인그레스 컨트롤러를 외부에 노출함.
```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
spec:
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30100
  - name: https
    protocol: TCP
    port: 443
    targetPort: 443
    nodePort: 30101
  selector:
    app.kubernetes.io/name: ingress-nginx
  type: NodePort
```

```shell
# 노드포트 서비스로 생성된 NGINX 인그레스 컨트롤러 
kubectl apply -f ~/_Book-k8sInfra/ch3/3.3.2/ingress.yaml

kubectl get services -n ingress-nginx

NAME                       TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
nginx-ingress-controller   NodePort   10.109.180.164   <none>        80:30100/TCP,443:30101/TCP   37m


#expose 명령어로 디플로이먼트도 서비스를 노출함.
#외부와 통신하기 위해 클러스터 내부에서만 사용하는 파드를 클러스터 외부에 노출할 수 있는 구역으로 옮김
# 내부와 외부 네트워크를 분리해 관리하는 DMZ와 유사한 기능

kubectl expose deployment in-hname-pod --name=hname-svc-default --port=80,443
service/hname-svc-default exposed

kubectl expose deployment in-ip-pod --name=ip-svc --port=80,443
service/ip-svc exposed

kubectl get services;
NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
hname-svc-default   ClusterIP   10.107.182.103   <none>        80/TCP,443/TCP   42m
ip-svc              ClusterIP   10.105.93.168    <none>        80/TCP,443/TCP   41m
kubernetes          ClusterIP   10.96.0.1        <none>        443/TCP          2d3h
```

##### 클라우드에서 쉽게 구성 가능한 로드밸런서

위의 연결방식들은 워커 노드의 노드포트를 통해 노드포트 서비스로 이동하고 이를 쿠버네티스 파드로 보내는 구조였음. 이 방식은 비효율적이므로 로드 백런스라는 서비스 타입을 제공함

로드 밸런서를 사용하려면 로드 밸런서를 이미 구현해 둔 서비스 업체의 도움을 받아 쿠버네티스 클러스터 외부에 구현해야하기 떄문.

클라우드에서 제공하는 쿠버네티스를 사용하고 있다면 다음과 같이 선언만 하면 됨. 그러면 쿠버네티스 클러스에 로드밸런서 서비스가 생성되 외부와 통신할 수 있는 IP가 부여되고 외부와 통신할 수 있으며 부하도 분산됨
```shell
#EKS, GKE, AKS에서만 가능
kubectl expose deployment ex-lb --type=LoadBalancer --name=ex-svc
```

##### 가상환경에서 로드밸런서를 사용

온프레미스에서 로드밸런서를 사용하려면 로드밸런서 서비스를 받아주는 구성이 필요한데 그걸 제공하는 MetalLB. MetalLB는 베어메탈로 구성된 로드밸런서에서를 사용할 수 있게 고안된 프로젝트

MetalLB는 기존의 L2네트워크(ARP/NDP)와 L3네트워크 (BGP)로 로드밸런서를 구성함

MetalLB 컨트롤러는 작동방식(프로토콜)을 정의하고 EXTERNAL-IP를 부여해 관리함. MetalLB스피커는 정해진 작동방식(L2/ARP, L3/BGP)에 따라 경로를 만들 수 있도록 네트워크 정보를 광고하고
수집해 각 파드의 경로를 제공함. 이때 L2는 스피커중에서 리더를 선출해 경로 제공을 총괄하게 함.

```shell
# 디플로이먼트를 이용해 2가지 파드를 생성함
kubectl create deployment lb-hname-pods --image=sysnet4admin/echo-hname 

kubectl scale deployment lb-hname-pods --replicas=3

kubectl create deployment lb-ip-pods --image=sysnet4admin/echo-ip 

kubectl scale deployment lb-ip-pods --replicas=3
```

MetalLB 오브젝트
```yaml
# All of sources From 
# - https://raw.githubusercontent.com/metallb/metallb/v0.8.3/manifests/metallb.yaml
# clone from above to sysnet4admin 

apiVersion: v1
kind: Namespace
metadata:
  labels:
    app: metallb
  name: metallb-system
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  labels:
    app: metallb
  name: speaker
  namespace: metallb-system
spec:
  allowPrivilegeEscalation: false
  allowedCapabilities:
  - NET_ADMIN
  - NET_RAW
  - SYS_ADMIN
  fsGroup:
    rule: RunAsAny
  hostNetwork: true
  hostPorts:
  - max: 7472
    min: 7472
  privileged: true
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  volumes:
  - '*'
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: metallb
  name: controller
  namespace: metallb-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: metallb
  name: speaker
  namespace: metallb-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: metallb
  name: metallb-system:controller
rules:
- apiGroups:
  - ''
  resources:
  - services
  verbs:
  - get
  - list
  - watch
  - update
- apiGroups:
  - ''
  resources:
  - services/status
  verbs:
  - update
- apiGroups:
  - ''
  resources:
  - events
  verbs:
  - create
  - patch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app: metallb
  name: metallb-system:speaker
rules:
- apiGroups:
  - ''
  resources:
  - services
  - endpoints
  - nodes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ''
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - extensions
  resourceNames:
  - speaker
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app: metallb
  name: config-watcher
  namespace: metallb-system
rules:
- apiGroups:
  - ''
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: metallb
  name: metallb-system:controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: metallb-system:controller
subjects:
- kind: ServiceAccount
  name: controller
  namespace: metallb-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: metallb
  name: metallb-system:speaker
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: metallb-system:speaker
subjects:
- kind: ServiceAccount
  name: speaker
  namespace: metallb-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app: metallb
  name: config-watcher
  namespace: metallb-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: config-watcher
subjects:
- kind: ServiceAccount
  name: controller
- kind: ServiceAccount
  name: speaker
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: metallb
    component: speaker
  name: speaker
  namespace: metallb-system
spec:
  selector:
    matchLabels:
      app: metallb
      component: speaker
  template:
    metadata:
      annotations:
        prometheus.io/port: '7472'
        prometheus.io/scrape: 'true'
      labels:
        app: metallb
        component: speaker
    spec:
      containers:
      - args:
        - --port=7472
        - --config=config
        env:
        - name: METALLB_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: METALLB_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        image: metallb/speaker:v0.8.2
        imagePullPolicy: IfNotPresent
        name: speaker
        ports:
        - containerPort: 7472
          name: monitoring
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_ADMIN
            - NET_RAW
            - SYS_ADMIN
            drop:
            - ALL
          readOnlyRootFilesystem: true
      hostNetwork: true
      nodeSelector:
        beta.kubernetes.io/os: linux
      serviceAccountName: speaker
      terminationGracePeriodSeconds: 0
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: metallb
    component: controller
  name: controller
  namespace: metallb-system
spec:
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      app: metallb
      component: controller
  template:
    metadata:
      annotations:
        prometheus.io/port: '7472'
        prometheus.io/scrape: 'true'
      labels:
        app: metallb
        component: controller
    spec:
      containers:
      - args:
        - --port=7472
        - --config=config
        image: metallb/controller:v0.8.2
        imagePullPolicy: IfNotPresent
        name: controller
        ports:
        - containerPort: 7472
          name: monitoring
        resources:
          limits:
            cpu: 100m
            memory: 100Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - all
          readOnlyRootFilesystem: true
      nodeSelector:
        beta.kubernetes.io/os: linux
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: controller
      terminationGracePeriodSeconds: 0

```

MetalLB 설정을 적용해야하는 ConfigMap을 사용함
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: nginx-ip-range
      protocol: layer2  #matalLB에서 제공하는 로드밸런서의 동작 방식
      addresses:
      - 192.168.1.11-192.168.1.13 #matalLB에서 제공하는 로드밸런서의 외부 주소
```

```shell
# MetalLB 배포
kubectl get pods -n metallb-system -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE   READINESS GATES
controller-5f98465b6b-z6fw9   1/1     Running   0          28s   172.16.132.15   w3-k8s   <none>           <none>
speaker-hrktd                 1/1     Running   0          28s   192.168.1.101   w1-k8s   <none>           <none>
speaker-l8q4h                 1/1     Running   0          28s   192.168.1.102   w2-k8s   <none>           <none>
speaker-n4rzw                 1/1     Running   0          28s   192.168.1.103   w3-k8s   <none>           <none>
speaker-z6pbv                 1/1     Running   0          28s   192.168.1.10    m-k8s    <none>           <none>

# 서비스 노출
kubectl expose deployment lb-hname-pods --type=LoadBalancer --name=lb-hname-svc --port=80
service/lb-hname-svc exposed

kubectl expose deployment lb-ip-pods --type=LoadBalancer --name=lb-ip-svc --port=80
service/lb-ip-svc exposed

#ConfigMap을 통해 부여한 IP를 확인
kubectl get services -o wide
NAME           TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGE     SELECTOR

kubernetes     ClusterIP      10.96.0.1       <none>         443/TCP        2d4h    <none>
lb-hname-svc   LoadBalancer   10.111.241.90   192.168.1.11   80:31906/TCP   5m47s   app=lb-hname-pods
lb-ip-svc      LoadBalancer   10.106.243.6    192.168.1.12   80:31514/TCP   5m19s   app=lb-ip-pods

```

##### 부하에 따라 자동으로 수를 조절하는 HPA(Horizontal Pod Autoscaler)

```shell
# 디플로이먼트 1개 생성
kubectl create deployment hpa-hname-pods --image=sysnet4admin/echo-hname

# hpa-hname-pods를 로드밸런서 서비스로 설정
kubectl expose deployment hpa-hname-pods --type=LoadBalancer --name=hpa-hname-svc --port=80

kubectl get services
NAME            TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)        AGE
hpa-hname-svc   LoadBalancer   10.102.195.132   192.168.1.11   80:31044/TCP   35s
kubernetes      ClusterIP      10.96.0.1        <none>         443/TCP        2d4h

# HPA 작동하려면 파드의 자원이 어느정도 사용되는지 확인해야함
kubectl top pods
Error from server (NotFound): the server could not find the requested resource (get services http:heapster:)

HPA가 자원을 요청할 떄 메트릭 서버를 통해 계측값을 전달받음. 오류는 메트릭 서버가 없기 떄문에 발생함

```

메트릭 서버 생성
```yaml
#Main_Source_From: 
# - https://github.com/kubernetes-sigs/metrics-server

#aggregated-metrics-reader.yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:aggregated-metrics-reader
  labels:
    rbac.authorization.k8s.io/aggregate-to-view: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
rules:
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods", "nodes"]
  verbs: ["get", "list", "watch"]

#auth-delegator.yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: metrics-server:system:auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system

#auth-reader.yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: metrics-server-auth-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system

#metrics-apiservice.yaml
---
apiVersion: apiregistration.k8s.io/v1beta1
kind: APIService
metadata:
  name: v1beta1.metrics.k8s.io
spec:
  service:
    name: metrics-server
    namespace: kube-system
  group: metrics.k8s.io
  version: v1beta1
  insecureSkipTLSVerify: true
  groupPriorityMinimum: 100
  versionPriority: 100

#metrics-server-deployment.yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: metrics-server
  namespace: kube-system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-server
  namespace: kube-system
  labels:
    k8s-app: metrics-server
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
  template:
    metadata:
      name: metrics-server
      labels:
        k8s-app: metrics-server
    spec:
      serviceAccountName: metrics-server
      volumes:
      # mount in tmp so we can safely use from-scratch images and/or read-only containers
      - name: tmp-dir
        emptyDir: {}
      containers:
      - name: metrics-server
        image: k8s.gcr.io/metrics-server-amd64:v0.3.6
        args:
        # Manually Add for lab env(Sysnet4admin/k8s)
        # skip tls internal usage purpose
          - --kubelet-insecure-tls
        # kubelet could use internalIP communication 
          - --kubelet-preferred-address-types=InternalIP
          - --cert-dir=/tmp
          - --secure-port=4443
        ports:
        - name: main-port
          containerPort: 4443
          protocol: TCP
        securityContext:
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
        imagePullPolicy: Always
        volumeMounts:
        - name: tmp-dir
          mountPath: /tmp
      nodeSelector:
        beta.kubernetes.io/os: linux
        kubernetes.io/arch: "amd64"

#metrics-server-service.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: metrics-server
  namespace: kube-system
  labels:
    kubernetes.io/name: "Metrics-server"
    kubernetes.io/cluster-service: "true"
spec:
  selector:
    k8s-app: metrics-server
  ports:
  - port: 443
    protocol: TCP
    targetPort: main-port

#resource-reader.yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: system:metrics-server
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - nodes
  - nodes/stats
  - namespaces
  - configmaps
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:metrics-server
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:metrics-server
subjects:
- kind: ServiceAccount
  name: metrics-server
  namespace: kube-system

```

```shell
kubectl create -f ~/_Book_k8sInfra/ch3/3.3.5/metrics-server.yaml
```